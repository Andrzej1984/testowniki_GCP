X1000
Your company's on-premises Apache Hadoop servers are approaching end-of-life, and IT has decided to migrate the cluster to Google Cloud Dataproc. A like-for- like migration of the cluster would require 50 TB of Google Persistent Disk per node. The CIO is concerned about the cost of using that much block storage. You want to minimize the storage cost of the migration. What should you do?
Put the data into Google Cloud Storage.
Use preemptible virtual machines (VMs) for the Cloud Dataproc cluster.
Tune the Cloud Dataproc cluster so that there is just enough disk for all data.
Migrate some of the cold data into Google Cloud Storage, and keep only the hot data in Persistent Disk.